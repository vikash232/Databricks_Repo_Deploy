{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyspark class Row from module sql\n",
    "from pyspark.sql import *\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# Create Example Data - Departments and Employees\n",
    "\n",
    "# Create the Departments\n",
    "department1 = Row(id='101', name='Computer Science')\n",
    "department2 = Row(id='102', name='Mechanical Engineering')\n",
    "department3 = Row(id='103', name='Theater and Drama')\n",
    "department4 = Row(id='104', name='Indoor Recreation')\n",
    "\n",
    "# Create the Employees\n",
    "Employee = Row(\"firstName\", \"lastName\", \"email\", \"salary\",\"deptId\")\n",
    "employee1 = Employee('michael', 'armbrust', 'no-reply@berkeley.edu', 100000,101)\n",
    "employee2 = Employee('xiangrui', 'meng', 'no-reply@stanford.edu', 120000,101)\n",
    "employee3 = Employee('matei', None, 'no-reply@waterloo.edu', 140000,102)\n",
    "employee4 = Employee(None, 'wendell', 'no-reply@berkeley.edu', 160000,102)\n",
    "employee5 = Employee('michael', 'jackson', 'no-reply@neverla.nd', 80000,103)\n",
    "employee6 = Employee('john', 'doe', 'no-reply@unclesame.usa.gov', 800000,101)\n",
    "\n",
    "depts = [department1, department1,department3,department4]\n",
    "employees = [employee1, employee2,employee3,employee4,employee5,employee6]\n",
    "\n",
    "deptsDF = spark.createDataFrame(depts)\n",
    "empsDF = spark.createDataFrame(employees)\n",
    "\n",
    "deptsDF.show()\n",
    "empsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from shutil import copyfile\n",
    "from pyspark.dbutils import DBUtils\n",
    "\n",
    "dbutils = DBUtils(spark.sparkContext)\n",
    "# print(dbutils.fs.ls(\"dbfs:/\"))\n",
    "\n",
    "global fileprefix\n",
    "global deptFilePath\n",
    "deptFilePath = \"dept-sample.json\"\n",
    "\n",
    "fileprefix = \"dbfs:/sample-data\"\n",
    "print('deptsDF count is : ', deptsDF.count())\n",
    "\n",
    "deptsDF.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").json(fileprefix+\".tmp\")\n",
    "partition_path = dbutils.fs.ls(fileprefix+\".tmp/\")\n",
    "path = [file for file in partition_path if file.name.endswith(\".json\")][0].path\n",
    "# path = re.sub('dbfs:', '/dbfs', path)\n",
    "deptFilePath = \"dbfs:/\" + deptFilePath\n",
    "# print('Path is : ', path)\n",
    "# print('deptFilePath is : ', deptFilePath)\n",
    "\n",
    "\n",
    "try:\n",
    "    dbutils.fs.cp(path, deptFilePath)\n",
    "    dbutils.fs.rm(path)\n",
    "    print(dbutils.fs.head(deptFilePath))\n",
    "\n",
    "except:\n",
    "    print(\"Unexpected error:\", sys.exc_info())\n",
    "    exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global empFilePath\n",
    "empFilePath = \"employee-sample.json\"\n",
    "\n",
    "print('empsDF count is : ', empsDF.count())\n",
    "\n",
    "empsDF.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").json(fileprefix+\".tmp\")\n",
    "partition_path = dbutils.fs.ls(fileprefix+\".tmp/\")\n",
    "path = [file for file in partition_path if file.name.endswith(\".json\")][0].path\n",
    "empFilePath = \"dbfs:/\" + empFilePath\n",
    "# print('Path is : ', path)\n",
    "# print('deptFilePath is : ', empFilePath)\n",
    "\n",
    "try:\n",
    "    dbutils.fs.cp(path, empFilePath)\n",
    "    dbutils.fs.rm(path)\n",
    "    print(dbutils.fs.head(empFilePath))\n",
    "except:\n",
    "    print(\"Unexpected error:\", sys.exc_info())\n",
    "    exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbutils.fs.rm(\"dept-sample.json.json\")\n",
    "# deptFilePath = re.sub('/dbfs', '', deptFilePath)\n",
    "df = spark.read.option(\"inferSchema\",True).json(deptFilePath)\n",
    "print('df count is : ', df.count())\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empFilePath = re.sub('/dbfs', '', empFilePath)\n",
    "print(empFilePath)\n",
    "empDf1 = spark.read.option(\"inferSchema\",True).json(empFilePath)\n",
    "print('empDf1 count is : ', empDf1.count())\n",
    "empDf1.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "name": "kk_test",
  "notebookId": 438072753591093
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
